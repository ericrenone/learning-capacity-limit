# Capacity Dynamics in Neural Optimization

The "Resolution" Limit of Simplified ModelsThe Restricted (Red) model demonstrates that hardware or software constraints—specifically quantization—act as a physical barrier to intelligence. Because this model rounds its parameters to two decimal places, it hits a "noise floor." No matter how much data you feed it, it cannot achieve a loss of zero because the true target ($1.2599...$) cannot be represented by its limited vocabulary. This concludes that model compression saves memory but inherently sacrifices the ability to find "perfect" truth.Momentum as a Filter for ChaosThe High-Capacity (Blue) model proves that Momentum acts as a sophisticated noise filter. In the simulation, the data is intentionally "noisy" (using noise_scale). The Restricted model reacts to every bump in the road, causing its path to look jagged and erratic. However, the High-Capacity model uses its "velocity" to glide through the noise, showing that a model with "memory" of where it has been is significantly more stable and efficient than one that only lives in the moment.The Stability of Information TopologyBy tracking Fisher Information, the simulation shows that "smarter" models create a more stable relationship with their data. The Restricted model experiences high, unpredictable spikes in information density because its rounding errors create "friction" against the data. The High-Capacity model, however, maintains a smoother trajectory through the information space. This suggests that increasing a model's capacity doesn't just make it more accurate; it makes the learning process itself more predictable and less prone to catastrophic failures.
